{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPAN6</th>\n",
       "      <th>TNMD</th>\n",
       "      <th>DPM1</th>\n",
       "      <th>SCYL3</th>\n",
       "      <th>C1orf112</th>\n",
       "      <th>FGR</th>\n",
       "      <th>CFH</th>\n",
       "      <th>FUCA2</th>\n",
       "      <th>GCLC</th>\n",
       "      <th>NFYA</th>\n",
       "      <th>...</th>\n",
       "      <th>AC092910.4</th>\n",
       "      <th>AC073611.1</th>\n",
       "      <th>AC136977.1</th>\n",
       "      <th>AC078856.1</th>\n",
       "      <th>AC008763.4</th>\n",
       "      <th>AL592295.6</th>\n",
       "      <th>AC006486.3</th>\n",
       "      <th>AL391628.1</th>\n",
       "      <th>AP006621.6</th>\n",
       "      <th>cancer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.7011</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>46.1397</td>\n",
       "      <td>1.8913</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>18.6466</td>\n",
       "      <td>4.3945</td>\n",
       "      <td>24.1591</td>\n",
       "      <td>6.7242</td>\n",
       "      <td>9.4371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0104</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>kidney chromophobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6275</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>45.5189</td>\n",
       "      <td>2.8743</td>\n",
       "      <td>0.4744</td>\n",
       "      <td>4.2348</td>\n",
       "      <td>1.4145</td>\n",
       "      <td>60.8153</td>\n",
       "      <td>15.4309</td>\n",
       "      <td>7.5267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.4262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>kidney chromophobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.9928</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>108.0270</td>\n",
       "      <td>4.8134</td>\n",
       "      <td>0.8010</td>\n",
       "      <td>4.5446</td>\n",
       "      <td>3.2850</td>\n",
       "      <td>33.1630</td>\n",
       "      <td>13.1085</td>\n",
       "      <td>12.1888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>kidney chromophobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.2605</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>27.5398</td>\n",
       "      <td>2.3258</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>4.5023</td>\n",
       "      <td>1.5078</td>\n",
       "      <td>47.2006</td>\n",
       "      <td>10.6721</td>\n",
       "      <td>5.0657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.1416</td>\n",
       "      <td>kidney chromophobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.8052</td>\n",
       "      <td>3.4134</td>\n",
       "      <td>88.0619</td>\n",
       "      <td>6.2415</td>\n",
       "      <td>1.2251</td>\n",
       "      <td>9.5310</td>\n",
       "      <td>30.6990</td>\n",
       "      <td>66.3841</td>\n",
       "      <td>12.9370</td>\n",
       "      <td>20.1289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.8857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>kidney chromophobe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60661 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    TSPAN6    TNMD      DPM1   SCYL3  C1orf112      FGR      CFH    FUCA2  \\\n",
       "0  19.7011  0.1125   46.1397  1.8913    0.2670  18.6466   4.3945  24.1591   \n",
       "1  31.6275  0.0000   45.5189  2.8743    0.4744   4.2348   1.4145  60.8153   \n",
       "2  35.9928  0.5207  108.0270  4.8134    0.8010   4.5446   3.2850  33.1630   \n",
       "3  18.2605  0.0921   27.5398  2.3258    0.3473   4.5023   1.5078  47.2006   \n",
       "4  68.8052  3.4134   88.0619  6.2415    1.2251   9.5310  30.6990  66.3841   \n",
       "\n",
       "      GCLC     NFYA  ...  AC092910.4  AC073611.1  AC136977.1  AC078856.1  \\\n",
       "0   6.7242   9.4371  ...         0.0      0.0641         0.0      1.0888   \n",
       "1  15.4309   7.5267  ...         0.0      0.0781         0.0      0.0000   \n",
       "2  13.1085  12.1888  ...         0.0      0.1049         0.0      0.0000   \n",
       "3  10.6721   5.0657  ...         0.0      0.1193         0.0      0.0000   \n",
       "4  12.9370  20.1289  ...         0.0      0.1141         0.0      0.0000   \n",
       "\n",
       "   AC008763.4  AL592295.6  AC006486.3  AL391628.1  AP006621.6  \\\n",
       "0         0.0      3.1977         0.0      0.0104      0.3953   \n",
       "1         0.0     15.4262         0.0      0.0042      0.4094   \n",
       "2         0.0     13.2312         0.0      0.1154      0.3050   \n",
       "3         0.0      6.4516         0.0      0.0141      0.1416   \n",
       "4         0.0     21.8857         0.0      0.0394      0.3547   \n",
       "\n",
       "          cancer_type  \n",
       "0  kidney chromophobe  \n",
       "1  kidney chromophobe  \n",
       "2  kidney chromophobe  \n",
       "3  kidney chromophobe  \n",
       "4  kidney chromophobe  \n",
       "\n",
       "[5 rows x 60661 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv(filepath_or_buffer='final_data_only_tpm.csv')\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('cancer_type', axis=1)\n",
    "y = data['cancer_type']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2,\n",
       "        2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 0, 0, 2, 1, 2, 1, 2, 2, 2, 0,\n",
       "        1, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 0, 2, 1, 1, 2, 1, 2, 1, 1, 2, 0, 2, 2,\n",
       "        0, 2, 1, 0, 0, 2, 2, 2, 0, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2,\n",
       "        2, 1, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,\n",
       "        1, 2, 2, 0, 0, 1, 2, 1, 2, 2, 2, 2, 0, 1, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2,\n",
       "        2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 1, 2, 1,\n",
       "        2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "import torch\n",
    "# Split the data into features (X) and target variable (y)\n",
    "\n",
    "\n",
    "map = {\n",
    "    \"kidney chromophobe\": 0,\n",
    "    \"kidney renal papillary cell carcinoma\": 1,\n",
    "    \"kidney renal clear cell carcinoma\": 2,\n",
    "}\n",
    "\n",
    "data1 = data.copy()\n",
    "data1['cancer_type'] = data1['cancer_type'].replace(map)\n",
    "\n",
    "X = data1.drop('cancer_type', axis=1)\n",
    "y = data1['cancer_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tens = torch.tensor(X_train.values).float()\n",
    "X_test_tens = torch.tensor(X_test.values).float()\n",
    "y_train_tens = torch.tensor(y_train.values)\n",
    "y_test_tens = torch.tensor(y_test.values)\n",
    "\n",
    "data1.head()\n",
    "\n",
    "y_test_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import dependencies\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Set the hyperparameters for data creation\n",
    "# NUM_CLASSES = 4\n",
    "# NUM_FEATURES = 2\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# # 1. Create multi-class data\n",
    "# X_blob, y_blob = make_blobs(n_samples=1000,\n",
    "#     n_features=NUM_FEATURES, # X features\n",
    "#     centers=NUM_CLASSES, # y labels \n",
    "#     cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
    "#     random_state=RANDOM_SEED\n",
    "# )\n",
    "\n",
    "# # 2. Turn data into tensors\n",
    "# X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "# y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "# print(X_blob[:5], y_blob[:5])\n",
    "\n",
    "# # 3. Split into train and test sets\n",
    "# X_train_tens, X_test_tens, y_train_tens, y_test_tens = train_test_split(X_blob,\n",
    "#     y_blob,\n",
    "#     test_size=0.2,\n",
    "#     random_state=RANDOM_SEED\n",
    "# )\n",
    "\n",
    "# # 4. Plot data\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Build model\n",
    "class MultiClassificationModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): Number of input features to the model.\n",
    "            out_features (int): Number of output features of the model\n",
    "              (how many classes there are).\n",
    "            hidden_units (int): Number of hidden units between layers, default 8.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            #nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            #nn.Sigmoid(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            #nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiClassificationModel(\n",
       "  (linear_layer_stack): Sequential(\n",
       "    (0): Linear(in_features=60660, out_features=8, bias=True)\n",
       "    (1): Linear(in_features=8, out_features=8, bias=True)\n",
       "    (2): Linear(in_features=8, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_FEATURES = 60660\n",
    "NUM_CLASSES = 3\n",
    "device = \"cpu\"\n",
    "\n",
    "# Create an instance of BlobModel and send it to the target device\n",
    "model = MultiClassificationModel(input_features=NUM_FEATURES, \n",
    "                    output_features=NUM_CLASSES, \n",
    "                    hidden_units=8).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.001) # exercise: try changing the learning rate here and seeing what happens to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.3309,  60.4203, -16.2986],\n",
       "        [  9.0187, 102.5659,  -3.7891],\n",
       "        [-23.3094,  65.5774, -31.0714],\n",
       "        [-46.3397,  62.3671, -45.2362],\n",
       "        [ 22.1226,  38.7476,  -7.6419]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n",
    "model(X_train_tens.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train_tens.to(device))[0].shape, NUM_CLASSES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.9766, 106.3172,  -9.6780],\n",
      "        [ -2.2327,  62.3234, -21.7038],\n",
      "        [-14.7883,  50.5043, -22.6019],\n",
      "        [  3.4967,   6.3571, -30.9968],\n",
      "        [-37.1034, -24.6716, -60.3328]], grad_fn=<SliceBackward0>)\n",
      "tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [9.1972e-29, 1.0000e+00, 3.2172e-37],\n",
      "        [4.4036e-29, 1.0000e+00, 1.7799e-32],\n",
      "        [5.4148e-02, 9.4585e-01, 5.6654e-17],\n",
      "        [3.9895e-06, 1.0000e+00, 3.2549e-16]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction logits with model\n",
    "y_logits = model(X_test_tens.to(device))\n",
    "\n",
    "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1) \n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the first sample output of the softmax activation function \n",
    "torch.sum(y_pred_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0.], grad_fn=<SelectBackward0>)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Which class does the model think is *most* likely at the index 0 sample?\n",
    "print(y_pred_probs[0])\n",
    "print(torch.argmax(y_pred_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 76.89359, Acc: 31.39% | Test Loss: 757577.50000, Test Acc: 10.19%\n",
      "Epoch: 1 | Loss: 798421.93750, Acc: 8.52% | Test Loss: 807218053120.00000, Test Acc: 10.19%\n",
      "Epoch: 2 | Loss: 821644296192.00000, Acc: 8.39% | Test Loss: 1806918533938090136829952.00000, Test Acc: 61.65%\n",
      "Epoch: 3 | Loss: 1724170907594259439812608.00000, Acc: 59.25% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 4 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 5 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 6 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 7 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 8 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 9 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 10 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 11 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 12 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 13 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 14 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 15 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 16 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 17 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 18 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 19 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 20 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 21 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 22 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 23 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 24 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 25 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 26 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 27 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 28 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 29 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 30 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 31 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 32 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 33 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 34 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 35 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 36 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 37 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 38 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 39 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 40 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 41 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 42 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 43 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 44 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 45 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 46 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 47 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 48 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 49 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 50 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 51 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 52 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 53 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 54 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 55 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 56 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 57 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 58 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 59 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 60 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 61 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 62 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 63 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 64 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 65 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 66 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 67 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 68 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 69 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 70 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 71 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 72 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 73 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 74 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 75 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 76 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 77 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 78 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 79 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 80 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 81 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 82 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 83 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 84 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 85 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 86 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 87 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 88 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 89 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 90 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 91 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 92 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 93 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 94 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 95 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 96 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 97 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 98 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n",
      "Epoch: 99 | Loss: nan, Acc: 8.52% | Test Loss: nan, Test Acc: 10.19%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train_tens, y_train_tens = X_train_tens.to(device), y_train_tens.to(device)\n",
    "X_test_tens, y_test_tens = X_test_tens.to(device), y_test_tens.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    y_logits = model(X_train_tens) # model outputs raw logits \n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "    # print(y_logits)\n",
    "    # 2. Calculate loss and accuracy\n",
    "    #print(\"####\", y_logits)\n",
    "    #print(\"####\", y_train_tens)\n",
    "    loss = loss_fn(y_logits, y_train_tens) \n",
    "    #print(\"####loss\", loss)\n",
    "    acc = accuracy_fn(y_true=y_train_tens,\n",
    "                      y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass\n",
    "      test_logits = model(X_test_tens)\n",
    "      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "      # 2. Calculate test loss and accuracy\n",
    "      test_loss = loss_fn(test_logits, y_test_tens)\n",
    "      test_acc = accuracy_fn(y_true=y_test_tens,\n",
    "                             y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n",
    "    if epoch+1 % 40 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_train_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss() \u001b[38;5;66;03m#torch.nn.MSELoss()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m----> 9\u001b[0m train_step \u001b[38;5;241m=\u001b[39m \u001b[43mmake_train_step\u001b[49m(\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     11\u001b[0m     dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[0;32m     12\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     14\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m val_step \u001b[38;5;241m=\u001b[39m make_val_step(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, dataloader\u001b[38;5;241m=\u001b[39mval_dataloader, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_train_step' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "lr = 0.01\n",
    "model = MultiClassificationModel(input_features=60660, output_features=3).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss() #torch.nn.MSELoss()\n",
    "n_epochs = 25\n",
    "\n",
    "train_step = make_train_step(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "val_step = make_val_step(\n",
    "    model=model, dataloader=val_dataloader, loss_fn=loss_fn, device=device\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_step()\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = val_step()\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{(epoch + 1):2}/{n_epochs:2}] | train loss: {train_loss:.4f} | validation loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
